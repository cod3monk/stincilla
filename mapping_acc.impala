fn compute_config(range_mask: (i32, i32), unroll: i32) -> ((i32, i32, i32), (i32, i32)) {
    fn compute_config_y (y_max: i32) -> i32 {
        if range_mask(1) > 1 { return(y_max) }
        for n in range(1, y_max+1) {
            if n * unroll >= range_mask(1) { return(n) }
        }
        y_max
    }

    let config_y_max = 4;   // must be 1 for CPU on Mac OS X
    let config_x_max = 128; // use 256 for AMD GPUs; 512 for Intel MIC

    let threads_y = compute_config_y(config_y_max);
    let threads_x = config_x_max / threads_y;

    let blocks_x = round_up(range_mask(0), threads_x) / threads_x;
    let blocks_y = round_up(range_mask(1), threads_y) / threads_y;

    ((threads_x, threads_y, 1), (blocks_x, blocks_y))
}

fn get_device_copy(img: Img) -> Img {
    let img_gpu = get_img(acc_alloc(acc_dev(), img.width * img.height * sizeof[f32]()), img.width, img.height);
    copy(img.buf, img_gpu.buf, img.width * img.height * sizeof[f32]());
    img_gpu
}

fn release_device_copy(img: Img) -> () {
    release(img.buf)
}

fn iteration_point(out: Img,
                   body: fn(i32, i32, Acc) -> ()
                  ) -> () {
    let out_gpu = get_img(acc_alloc(acc_dev(), out.width * out.height * sizeof[f32]()), out.width, out.height);

    let unroll = 1;
    let grid   = (out.width, out.height/unroll, 1);
    let block  = (128, 1, 1);

    for benchmark_acc() {
        with acc(acc_dev(), grid, block) @{
            let gid_x = acc_gidx();
            let gid_y = acc_tidy() + acc_bdimy() * acc_bidy() * unroll;
            let out_acc = get_acc(out_gpu);

            for i in range(0, unroll) {
                body(gid_x, gid_y + i * acc_bdimy(), out_acc);
            }
        }

        acc_sync(acc_dev());
    }

    copy(out_gpu.buf, out.buf, out.width * out.height * sizeof[f32]());
    release(out_gpu.buf);
}

fn iteration(out: Img, arr: Img, mask: Mask,
             bh_lower: fn(i32, i32, i32, fn(f32)) -> i32, bh_upper: fn(i32, i32, i32, fn(f32)) -> i32,
             body: fn(i32, i32, Acc, Acc, Mask) -> ()
            ) -> () {
    let arr_gpu = get_img(acc_alloc(acc_dev(), arr.width * arr.height * sizeof[f32]()), arr.width, arr.height);
    let out_gpu = get_img(acc_alloc(acc_dev(), out.width * out.height * sizeof[f32]()), out.width, out.height);
    copy(arr.buf, arr_gpu.buf, arr.width * arr.height * sizeof[f32]());

    let unroll = 1;
    let config = compute_config((mask.size_x/2, mask.size_y/2), unroll);
    let grid   = (out.width, out.height/unroll, 1);
    let block  = config(0);
    let get_acc_bh = if acc_use_tex() { get_acc_tex_bh } else { get_acc_bh };

    for benchmark_acc() {
        with acc(acc_dev(), grid, block) @{
            let gid_x = acc_gidx();
            let gid_y = acc_tidy() + acc_bdimy() * acc_bidy() * unroll;
            let arr_acc = get_acc_bh(arr_gpu, 10, bh_lower, bh_upper);
            let out_acc = get_acc(out_gpu);

            for i in range(0, unroll) {
                body(gid_x, gid_y + i * acc_bdimy(), out_acc, arr_acc, mask);
            }
        }
        acc_sync(acc_dev());
    }

    copy(out_gpu.buf, out.buf, out.width * out.height * sizeof[f32]());
    release(arr_gpu.buf);
    release(out_gpu.buf);
}

fn iteration_bounds(out: Img, arr: Img, mask: Mask,
                    bh_lower: fn(i32, i32, i32, fn(f32)) -> i32, bh_upper: fn(i32, i32, i32, fn(f32)) -> i32,
                    body: fn(i32, i32, Acc, Acc, Mask) -> ()
                   ) -> () {
    let arr_gpu = get_img(acc_alloc(acc_dev(), arr.width * arr.height * sizeof[f32]()), arr.width, arr.height);
    let out_gpu = get_img(acc_alloc(acc_dev(), out.width * out.height * sizeof[f32]()), out.width, out.height);
    copy(arr.buf, arr_gpu.buf, arr.width * arr.height * sizeof[f32]());

    // compute the number of blocks required for boundary handling
    let unroll = 1;
    let config = compute_config((mask.size_x/2, mask.size_y/2), unroll);
    let block  = config(0);
    let bx     = config(1)(0);
    let by     = config(1)(1);
    let get_acc_bh = if acc_use_tex() { get_acc_tex_bh } else { get_acc_bh };

    // define if we want to generate one big kernel or multiple kernels
    let big_kernel = false;

    if big_kernel {
        let grid = (out.width, out.height/unroll, 1);

        for benchmark_acc() {
            with acc(acc_dev(), grid, block) @{
                let bid_x   = acc_bidx();
                let bid_y   = acc_bidy();
                let bdim_y  = acc_bdimy();
                let gdim_x  = acc_gdimx();
                let gdim_y  = acc_gdimy();
                let gid_x   = acc_gidx();
                let gid_y   = acc_tidy() + acc_bdimy() * acc_bidy() * unroll;
                let out_acc = get_acc(out_gpu);

                // 0 --- 1 --- 2
                // 3 --- 4 --- 5
                // 6 --- 7 --- 8
                if bid_x < bx && bid_y < by @{                          // top-left: 0
                    let arr_acc = get_acc_bh(arr_gpu, 0, bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, arr_acc, mask);
                    }
                } else if bid_x >= gdim_x-bx && bid_y < by @{           // top-right: 2
                    let arr_acc = get_acc_bh(arr_gpu, 2, bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, arr_acc, mask);
                    }
                } else if bid_y < by @{                                 // top: 1
                    let arr_acc = get_acc_bh(arr_gpu, 1, bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, arr_acc, mask);
                    }
                } else if bid_y >= gdim_y-by && bid_x < bx @{           // bottom-left: 6
                    let arr_acc = get_acc_bh(arr_gpu, 6, bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, arr_acc, mask);
                    }
                } else if bid_y >= gdim_y-by && bid_x >= gdim_x-bx @{   // bottom-right: 8
                    let arr_acc = get_acc_bh(arr_gpu, 8, bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, arr_acc, mask);
                    }
                } else if bid_y >= gdim_y-by @{                         // bottom: 7
                    let arr_acc = get_acc_bh(arr_gpu, 7, bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, arr_acc, mask);
                    }
                } else if bid_x >= gdim_x-bx @{                         // right: 5
                    let arr_acc = get_acc_bh(arr_gpu, 5, bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, arr_acc, mask);
                    }
                } else if bid_x < bx @{                                 // left: 3
                    let arr_acc = get_acc_bh(arr_gpu, 3, bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, arr_acc, mask);
                    }
                } else @{                                               // center: 4
                    let arr_acc = get_acc_bh(arr_gpu, 4, bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, arr_acc, mask);
                    }
                }
            }
            acc_sync(acc_dev());
        }
    } else {
        // bounds account for unroll factor
        let hu = out.height/unroll;
        let Bounds2D = [
            (0                      , bx*block(0)            , 0               , by*block(1)),
            (bx*block(0)            , out.width - bx*block(0), 0               , by*block(1)),
            (out.width - bx*block(0), out.width              , 0               , by*block(1)),

            (0                      , bx*block(0)            , by*block(1)     , hu - by*block(1)),
            (bx*block(0)            , out.width - bx*block(0), by*block(1)     , hu - by*block(1)),
            (out.width - bx*block(0), out.width              , by*block(1)     , hu - by*block(1)),

            (0                      , bx*block(0)            , hu - by*block(1), hu),
            (bx*block(0)            , out.width - bx*block(0), hu - by*block(1), hu),
            (out.width - bx*block(0), out.width              , hu - by*block(1), hu)
        ];

        for region in @range(0, 9) {
            let bounds = Bounds2D(region);
            let grid   = (bounds(1) - bounds(0), bounds(3) - bounds(2), 1);

            let arr_acc = get_acc_bh(arr_gpu, region, bh_lower, bh_upper);
            let out_acc = get_acc(out_gpu);

            for benchmark_acc() {
                with acc(acc_dev(), grid, block) @{
                    let gid_x = bounds(0) +
                                acc_tidx() + acc_bdimx() * acc_bidx();
                    let gid_y = bounds(2) * unroll +
                                acc_tidy() + acc_bdimy() * acc_bidy() * unroll;

                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * acc_bdimy(), out_acc, arr_acc, mask);
                    }
                }
                acc_sync(acc_dev());
            }
        }
    }

    copy(out_gpu.buf, out.buf, out.width * out.height * sizeof[f32]());
    release(arr_gpu.buf);
    release(out_gpu.buf);
}

fn iteration_advanced(out: Img, arr: Img, mask: Mask,
                      bh_lower: fn(i32, i32, i32, fn(f32)) -> i32, bh_upper: fn(i32, i32, i32, fn(f32)) -> i32,
                      body: fn(i32, i32, Acc, Acc, Mask) -> ()
                     ) -> () {
    let arr_gpu = get_img(acc_alloc(acc_dev(), arr.width * arr.height * sizeof[f32]()), arr.width, arr.height);
    let out_gpu = get_img(acc_alloc(acc_dev(), out.width * out.height * sizeof[f32]()), out.width, out.height);
    copy(arr.buf, arr_gpu.buf, arr.width * arr.height * sizeof[f32]());

    let unroll = 1;
    let config = compute_config((mask.size_x/2, mask.size_y/2), unroll);
    let grid   = (out.width, out.height/unroll, 1);
    let block  = config(0);
    let get_acc_bh = if acc_use_tex() { get_acc_tex_bh } else { get_acc_bh };

    // compute number of steps required to stage data to shared memory
    let range_row = mask.size_x / 2;
    let range_col = mask.size_y / 2;
    let steps_x   = 2;
    let offset_y  = if (mask.size_y-1)%block(1) == 0 { 0 } else { 1 };
    let steps_y   = unroll + (mask.size_y-1)/block(1) + offset_y;

    for benchmark_acc() {
        with acc(acc_dev(), grid, block) @{
            let tid_x  = acc_tidx();
            let tid_y  = acc_tidy();
            let bdim_x = acc_bdimx();
            let bdim_y = acc_bdimy();
            let gid_x  = acc_gidx();
            let gid_y  = acc_tidy() + acc_bdimy() * acc_bidy() * unroll;

            let spm_stride =          block(0) + 2 * range_row;
            let spm_height = unroll * block(1) + 2 * range_col;
            let spm        = reserve_shared[f32](spm_stride * spm_height);

            for y in range(0, steps_y) {
                let lid_y = tid_y             + y*bdim_y;
                let idx_y = gid_y - range_col + y*bdim_y;
                for x in range(0, steps_x) {
                    let lid_x = tid_x             + x*bdim_x;
                    let idx_x = gid_x - range_row + x*bdim_x;

                    if lid_x < spm_stride && lid_y < spm_height @{
                        let gpu_acc = get_acc_bh(arr_gpu, 10, bh_lower, bh_upper); // TODO: set region!
                        let spm_acc = get_acc_shared(spm, spm_stride, spm_height);
                        spm_acc.write(lid_x, lid_y, gpu_acc.read(idx_x, idx_y));
                    }
                }
            }

            acc_barrier();

            for i in range(0, unroll) {
                // index space: block
                let out_acc =        get_acc_offset(out_gpu,                     acc_bdimx() * acc_bidx(), acc_bdimy() * acc_bidy() * unroll + i * bdim_y);
                let arr_acc = get_acc_offset_shared(spm, spm_stride, spm_height, range_row,                range_col + i * bdim_y);
                body(tid_x, tid_y, out_acc, arr_acc, mask);
            }
        }
        acc_sync(acc_dev());
    }

    copy(out_gpu.buf, out.buf, out.width * out.height * sizeof[f32]());
    release(arr_gpu.buf);
    release(out_gpu.buf);
}

fn iteration_sep(out: Img, arr: Img, mask_row: MaskSep, mask_col: MaskSep,
                 bh_lower: fn(i32, i32, i32, fn(f32)) -> i32, bh_upper: fn(i32, i32, i32, fn(f32)) -> i32,
                 body: fn(i32, i32, Acc, Acc, MaskSep, bool) -> ()
                ) -> () {
    let arr_gpu = get_img(acc_alloc(acc_dev(), arr.width * arr.height * sizeof[f32]()), arr.width, arr.height);
    let out_gpu = get_img(acc_alloc(acc_dev(), out.width * out.height * sizeof[f32]()), out.width, out.height);
    let tmp_gpu = get_img(acc_alloc(acc_dev(), out.width * out.height * sizeof[f32]()), out.width, out.height);
    copy(arr.buf, arr_gpu.buf, arr.width * arr.height * sizeof[f32]());

    let unroll = 1;
    let grid   = (out.width, out.height/unroll, 1);
    let config_row = compute_config((mask_row.size/2, 1), unroll);
    let config_col = compute_config((1, mask_col.size/2), unroll);
    let block_row  = config_row(0);
    let block_col  = config_col(0);
    let get_acc_bh = if acc_use_tex() { get_acc_tex_bh } else { get_acc_bh };

    for benchmark_acc() {
        with acc(acc_dev(), grid, block_col) @{
            let gid_x = acc_gidx();
            let gid_y = acc_tidy() + acc_bdimy() * acc_bidy() * unroll;
            let is_row = false;

            let arr_acc = get_acc_bh(arr_gpu, 10, bh_lower, bh_upper);
            let tmp_acc = get_acc(tmp_gpu);

            for i in range(0, unroll) {
                body(gid_x, gid_y + i * acc_bdimy(), tmp_acc, arr_acc, mask_col, is_row);
            }
        }
        acc_sync(acc_dev());
    }
    for benchmark_acc() {
        with acc(acc_dev(), grid, block_row) @{
            let gid_x = acc_gidx();
            let gid_y = acc_tidy() + acc_bdimy() * acc_bidy() * unroll;
            let is_row = true;

            let tmp_acc = get_acc_bh(tmp_gpu, 10, bh_lower, bh_upper);
            let out_acc = get_acc(out_gpu);

            for i in range(0, unroll) {
                body(gid_x, gid_y + i * acc_bdimy(), out_acc, tmp_acc, mask_row, is_row);
            }
        }
        acc_sync(acc_dev());
    }

    copy(out_gpu.buf, out.buf, out.width * out.height * sizeof[f32]());
    release(arr_gpu.buf);
    release(out_gpu.buf);
    release(tmp_gpu.buf);
}

fn iteration_sep_bounds(out: Img, arr: Img, mask_row: MaskSep, mask_col: MaskSep,
                        bh_lower: fn(i32, i32, i32, fn(f32)) -> i32, bh_upper: fn(i32, i32, i32, fn(f32)) -> i32,
                        body: fn(i32, i32, Acc, Acc, MaskSep, bool) -> ()
                       ) -> () {
    let arr_gpu = get_img(acc_alloc(acc_dev(), arr.width * arr.height * sizeof[f32]()), arr.width, arr.height);
    let out_gpu = get_img(acc_alloc(acc_dev(), out.width * out.height * sizeof[f32]()), out.width, out.height);
    let tmp_gpu = get_img(acc_alloc(acc_dev(), out.width * out.height * sizeof[f32]()), out.width, out.height);
    copy(arr.buf, arr_gpu.buf, arr.width * arr.height * sizeof[f32]());

    // compute the number of blocks required for boundary handling
    let unroll = 1;
    let config_row = compute_config((mask_row.size/2, 1), unroll);
    let config_col = compute_config((1, mask_col.size/2), unroll);
    let block_row  = config_row(0);
    let block_col  = config_col(0);

    let bx         = config_row(1)(0);
    let by         = config_col(1)(1);
    let get_acc_bh = if acc_use_tex() { get_acc_tex_bh } else { get_acc_bh };

    let Region2DCol = [ 1, 4, 7 ];
    let Region2DRow = [ 3, 4, 5 ];

    // define if we want to generate one big kernel or multiple kernels
    let big_kernel = false;

    if big_kernel {
        let grid = (out.width, out.height/unroll, 1);

        // column component
        for benchmark_acc() {
            let tmp_acc = get_acc(tmp_gpu);
            with acc(acc_dev(), grid, block_col) @{
                let bid_y  = acc_bidy();
                let bdim_y = acc_bdimy();
                let gdim_y = acc_gdimy();
                let gid_x  = acc_gidx();
                let gid_y  = acc_tidy() + acc_bdimy() * acc_bidy() * unroll;
                let is_row = false;

                if bid_y < by @{                                // top: 1
                    let arr_acc = get_acc_bh(arr_gpu, Region2DCol(0), bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, tmp_acc, arr_acc, mask_col, is_row);
                    }
                } else if bid_y >= gdim_y-by @{                 // bottom: 7
                    let arr_acc = get_acc_bh(arr_gpu, Region2DCol(2), bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, tmp_acc, arr_acc, mask_col, is_row);
                    }
                } else @{                                       // center: 4
                    let arr_acc = get_acc_bh(arr_gpu, Region2DCol(1), bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, tmp_acc, arr_acc, mask_col, is_row);
                    }
                }
            }
            acc_sync(acc_dev());
        }

        // row component
        for benchmark_acc() {
            with acc(acc_dev(), grid, block_row) @{
                let bid_x   = acc_bidx();
                let bdim_y  = acc_bdimy();
                let gdim_x  = acc_gdimx();
                let gid_x   = acc_gidx();
                let gid_y   = acc_tidy() + acc_bdimy() * acc_bidy() * unroll;
                let is_row  = true;
                let out_acc = get_acc(out_gpu);

                if bid_x < bx @{                                // left: 3
                    let tmp_acc = get_acc_bh(tmp_gpu, Region2DRow(0), bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, tmp_acc, mask_row, is_row);
                    }
                } else if bid_x >= gdim_x-bx @{                 // right: 5
                    let tmp_acc = get_acc_bh(tmp_gpu, Region2DRow(2), bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, tmp_acc, mask_row, is_row);
                    }
                } else @{                                       // center: 4
                    let tmp_acc = get_acc_bh(tmp_gpu, Region2DRow(1), bh_lower, bh_upper);
                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * bdim_y, out_acc, tmp_acc, mask_row, is_row);
                    }
                }
            }
            acc_sync(acc_dev());
        }
    } else {
        let hu = out.height / unroll;
        let Bounds2DCol = [
            (0                   , by*block_col(1)),
            (by*block_col(1)     , hu - by*block_col(1)),
            (hu - by*block_col(1), hu)
        ];

        for iter in @range(0, 3) {
            let region = Region2DCol(iter);
            let bounds = Bounds2DCol(iter);
            let grid   = (out.width, bounds(1) - bounds(0), 1);

            let arr_acc = get_acc_bh(arr_gpu, region, bh_lower, bh_upper);
            let tmp_acc = get_acc(tmp_gpu);

            for benchmark_acc() {
                with acc(acc_dev(), grid, block_col) @{
                    let gid_x = acc_gidx();
                    let gid_y = bounds(0) * unroll +
                                acc_tidy() + acc_bdimy() * acc_bidy() * unroll;
                    let is_row = false;

                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * acc_bdimy(), tmp_acc, arr_acc, mask_col, is_row);
                    }
                }
                acc_sync(acc_dev());
            }
        }

        let Bounds2DRow = [
            (0                          , bx*block_row(0)),
            (bx*block_row(0)            , out.width - bx*block_row(0)),
            (out.width - bx*block_row(0), out.width)
        ];

        for iter in @range(0, 3) {
            let region = Region2DRow(iter);
            let bounds = Bounds2DRow(iter);
            let grid   = (bounds(1) - bounds(0), out.height/unroll, 1);

            let tmp_acc = get_acc_bh(tmp_gpu, region, bh_lower, bh_upper);
            let out_acc = get_acc(out_gpu);

            for benchmark_acc() {
                with acc(acc_dev(), grid, block_row) @{
                    let gid_x = bounds(0) + acc_gidx();
                    let gid_y = acc_tidy() + acc_bdimy() * acc_bidy() * unroll;
                    let is_row = true;

                    for i in range(0, unroll) {
                        body(gid_x, gid_y + i * acc_bdimy(), out_acc, tmp_acc, mask_row, is_row);
                    }
                }
                acc_sync(acc_dev());
            }
        }
    }

    copy(out_gpu.buf, out.buf, out.width * out.height * sizeof[f32]());
    release(arr_gpu.buf);
    release(out_gpu.buf);
    release(tmp_gpu.buf);
}

fn get_vector_length() -> int { 1 }
fn get_thread_number() -> int { 4 }
fn outer_loop_step(lower: int, upper: int, step: int, body: fn(int, fn())) -> () {
    for i in parallel(@get_thread_number(), 0, (upper-lower)/step) {
        body(i * step + lower);
    }
}
fn outer_loop(lower: int, upper: int, body: fn(int, fn())) -> () {
    for i in parallel(@get_thread_number(), lower, upper) {
        body(i);
    }
}
fn inner_loop(lower: int, upper: int, body: fn(int, fn())) -> () {
    if lower < upper {
        body(lower);
        inner_loop(lower+1, upper, body, return)
    }
}

struct CPUBounds {
    Bounds2DCol    : [(i32, i32) * 3],
    Region2DCol    : [i32        * 3],
    Parallelize    : [i32        * 3],

    Bounds2DRow    : [(i32, i32) * 3],
    Region2DRow    : [i32        * 3],
}

struct GPUBounds {
  Bounds2DCol : (i32, i32)      ,
  BlockCol    : (i32, i32, i32) ,

  Bounds2DRow : [(i32, i32) * 3],
  Region2DRow : [i32        * 3],
  BlockRow    : (i32, i32, i32)
}

// TODO find out what happens when you map/unmape the cpu/gpu regions
fn compute_bounds(img_width: i32, img_height: i32, mask_width: i32, mask_height: i32,
                       unroll: i32) -> (CPUBounds, GPUBounds) {
    let mut cpu_bounds : CPUBounds;
    let mut gpu_bounds : GPUBounds;

    let quar_width  = img_width  / 4;
    let quar_height = img_height / 4;
    let half_width  = img_width  / 2;
    let half_height = img_height / 2;

    let bhy = round_up(mask_height / 2, unroll);
    let bhx = round_up(mask_width  / 2, get_vector_length());

    cpu_bounds.Bounds2DCol = [
        (0               , bhy),
        (bhy             , (img_height - 2*bhy)/2),
        (img_height - bhy, img_height)
    ];

    cpu_bounds.Region2DCol = [1, 4, 7];
    cpu_bounds.Parallelize = [0, 1, 0];

    cpu_bounds.Bounds2DRow = [
        (0              , bhx),
        (bhx            , img_width - bhx),
        (img_width - bhx, img_width)
    ];

    cpu_bounds.Region2DRow = [3, 4, 5];

    let config_row = compute_config((mask_width/2, 1), unroll);
    let config_col = compute_config((1, mask_height/2), unroll);

    gpu_bounds.BlockRow  = config_row(0);
    gpu_bounds.BlockCol  = config_col(0);

    let bx = config_row(1)(0);

    gpu_bounds.Bounds2DCol = ((img_height - 2*bhy)/2, img_height - bhy);

    gpu_bounds.Region2DRow = [ 3, 4, 5 ];
    gpu_bounds.Bounds2DRow = [
          (0                                    , bx*gpu_bounds.BlockRow(0)),
          (bx*gpu_bounds.BlockRow(0)            , img_width - bx*gpu_bounds.BlockRow(0)),
          (img_width - bx*gpu_bounds.BlockRow(0), img_width)
      ];

    (cpu_bounds, gpu_bounds)
}

fn iteration_sep_hetero(out_gpu: Img, arr_gpu: Img, mask_row: MaskSep, mask_col: MaskSep,
                        bh_lower: fn(i32, i32, i32, fn(f32)) -> i32, bh_upper: fn(i32, i32, i32, fn(f32)) -> i32,
                        body: fn(i32, i32, Acc, Acc, MaskSep, bool) -> ()
                       ) -> () {
    //let arr_gpu = get_img(acc_alloc(acc_dev(), arr.width * arr.height * sizeof[f32]()), arr.width, arr.height);
    //let out_gpu = get_img(acc_alloc(acc_dev(), out.width * out.height * sizeof[f32]()), out.width, out.height);
    let tmp_gpu = get_img(acc_alloc(acc_dev(), out_gpu.width * out_gpu.height * sizeof[f32]()),
                          out_gpu.width, out_gpu.height);
    //copy(arr.buf, arr_gpu.buf, arr.width * arr.height * sizeof[f32]());

    ///
    /// Process CPU part
    ///
    let unroll = 1;
    let bounds = compute_bounds(arr_gpu.width, arr_gpu.height, mask_row.size, mask_col.size, unroll);
    let cpu_bounds = bounds(0);
    let gpu_bounds = bounds(1);

    // TODO verify these calculations when unroll > 1
    /*
    for i in range(0, 3) {
        let bounds_col = cpu_bounds.Bounds2DCol(i);
        let start = bounds_col(0) * out_gpu.width * unroll;
        let size = (bounds_col(1) - bounds_col(0)) * out_gpu.width * unroll;
        acc_map_region(acc_dev(), out_gpu.buf, start, size);
    }
    */

    //for benchmark_cpu() {
    fn body_cpu() -> () {
        for iter_col in @range(0, 3) {
            let bounds_col = cpu_bounds.Bounds2DCol(iter_col);
            let region_col = cpu_bounds.Region2DCol(iter_col);
            let outer_col = if cpu_bounds.Parallelize(iter_col) == 1 { outer_loop_step } else { range_step };
            let inner_col = if cpu_bounds.Parallelize(iter_col) == 1 { inner_loop } else { range };

            for  y in $outer_col(bounds_col(0), bounds_col(1), unroll) @{
                // allocate temporary array per thread
                let tmp = alloc_cpu(out_gpu.width * unroll * sizeof[f32]());

                for x in $inner_col(0, out_gpu.width) @{
                    let is_row = false;

                    let arr_acc = get_acc_bh_offset(arr_gpu, 0, y, region_col, bh_lower, bh_upper);
                    let tmp_acc = get_acc_memory(bitcast[&[f32]](tmp.data), out_gpu.width, unroll);
                    for i in range(0, unroll) {
                        body(x, i, tmp_acc, arr_acc, mask_col, is_row);
                    }
                }
                for iter_row in range(0, 3) {
                    let idx = iter_row;
                    let bounds_row = cpu_bounds.Bounds2DRow(idx);
                    let region_row = cpu_bounds.Region2DRow(idx);
                    let inner_row = if region_row == 4 { inner_loop } else { range };
                    for x in $inner_row(bounds_row(0), bounds_row(1)) @{
                        let is_row = true;

                        let tmp_acc = get_acc_bh_memory(bitcast[&[f32]](tmp.data), out_gpu.width, unroll,
                                                        region_row, bh_lower, bh_upper);
                        let out_acc = get_acc_offset(out_gpu, 0, y);
                        for i in range(0, unroll) {
                            body(x, i, out_acc, tmp_acc, mask_row, is_row);
                        }
                    }
                }

                release(tmp);
            }
        }
    }

    ///
    /// Process GPU part
    ///
    let grid   = (out_gpu.width, gpu_bounds.Bounds2DCol(1) - gpu_bounds.Bounds2DCol(0), 1);
    let gpu_height = gpu_bounds.Bounds2DCol(1) - gpu_bounds.Bounds2DCol(0);

    let arr_acc = get_acc_bh(arr_gpu, 4, bh_lower, bh_upper);
    let tmp_acc = get_acc(tmp_gpu);

    //for benchmark_acc() {
    fn body_acc1() -> () {
        with acc(acc_dev(), grid, gpu_bounds.BlockCol) @{
            let gid_x = acc_gidx();
            let gid_y = gpu_bounds.Bounds2DCol(0) * unroll +
                        acc_tidy() + acc_bdimy() * acc_bidy() * unroll;
            let is_row = false;

            for i in range(0, unroll) {
                body(gid_x, gid_y + i * acc_bdimy(), tmp_acc, arr_acc, mask_col, is_row);
            }
        }
    }

    fn body_acc2() -> () {
        for iter in @range(0, 3) {
            let region = gpu_bounds.Region2DRow(iter);
            let cur_bounds = gpu_bounds.Bounds2DRow(iter);
            let grid   = (cur_bounds(1) - cur_bounds(0), gpu_height/unroll, 1);

            let tmp_acc = get_acc_bh(tmp_gpu, region, bh_lower, bh_upper);
            let out_acc = get_acc(out_gpu);

            with acc(acc_dev(), grid, gpu_bounds.BlockRow) @{
                let gid_x = cur_bounds(0) + acc_gidx();
                let gid_y = gpu_bounds.Bounds2DCol(0) + acc_tidy() + acc_bdimy() * acc_bidy() * unroll;
                let is_row = true;

                for i in range(0, unroll) {
                    body(gid_x, gid_y + i * acc_bdimy(), out_acc, tmp_acc, mask_row, is_row);
                }
            }
        }
        //acc_sync(acc_dev());
    }

    benchmark_hetero(body_acc1, body_acc2, body_cpu);

    /*
    for i in range(0, 3) {
        let bounds_col = cpu_bounds.Bounds2DCol(i);
        let start = bounds_col(0) * out_gpu.width * unroll;
        let size = (bounds_col(1) - bounds_col(0)) * out_gpu.width * unroll;
        acc_unmap_region(acc_dev(), out_gpu.buf, start);
    }
    */

    //copy(out_gpu.buf, out.buf, out.width * out.height * sizeof[f32]());
    //release(arr_gpu.buf);
    //release(out_gpu.buf);
    release(tmp_gpu.buf);
}

fn iteration_sep_advanced(out: Img, arr: Img, mask_row: MaskSep, mask_col: MaskSep,
                          bh_lower: fn(i32, i32, i32, fn(f32)) -> i32, bh_upper: fn(i32, i32, i32, fn(f32)) -> i32,
                          body: fn(i32, i32, Acc, Acc, MaskSep, bool) -> ()
                         ) -> () {
    let arr_gpu = get_img(acc_alloc(acc_dev(), arr.width * arr.height * sizeof[f32]()), arr.width, arr.height);
    let out_gpu = get_img(acc_alloc(acc_dev(), out.width * out.height * sizeof[f32]()), out.width, out.height);
    copy(arr.buf, arr_gpu.buf, arr.width * arr.height * sizeof[f32]());

    let unroll = 1;
    let config = compute_config((mask_row.size/2, mask_col.size/2), unroll);
    let grid   = (out.width, out.height/unroll, 1);
    let block  = config(0);
    let get_acc_bh = if acc_use_tex() { get_acc_tex_bh } else { get_acc_bh };

    // compute number of steps required to stage data to shared memory
    let range_row = mask_row.size / 2;
    let range_col = mask_col.size / 2;
    let offset_y  = if (mask_col.size-1)%block(1) == 0 { 0 } else { 1 };
    let steps_x   = 2;
    let steps_y   = unroll + (mask_col.size-1)/block(1) + offset_y;

    for benchmark_acc() {
        with acc(acc_dev(), grid, block) @{
            let tid_x  = acc_tidx();
            let tid_y  = acc_tidy();
            let bdim_x = acc_bdimx();
            let bdim_y = acc_bdimy();
            let gid_x  = acc_gidx();
            let gid_y  = acc_tidy() + acc_bdimy() * acc_bidy() * unroll;

            let spm_stride     =          block(0) + 2 * range_row;
            let spm_height_col = unroll * block(1) + 2 * range_col;
            let spm_height_row = unroll * block(1);
            let spm_col = reserve_shared[f32](spm_stride * spm_height_col);
            let spm_row = reserve_shared[f32](spm_stride * spm_height_row);

            for y in range(0, steps_y) {
                let lid_y = tid_y             + y*bdim_y;
                let idx_y = gid_y - range_col + y*bdim_y;
                for x in range(0, steps_x) {
                    let lid_x = tid_x             + x*bdim_x;
                    let idx_x = gid_x - range_row + x*bdim_x;

                    if lid_x < spm_stride && lid_y < spm_height_col @{
                        let gpu_acc = get_acc_bh(arr_gpu, 10, bh_lower, bh_upper); // TODO: set region!
                        let spm_acc = get_acc_shared(spm_col, spm_stride, spm_height_col);
                        spm_acc.write(lid_x, lid_y, gpu_acc.read(idx_x, idx_y));
                    }
                }
            }

            acc_barrier();

            for i in range(0, unroll) {
                let is_row = false;
                {
                    // index space: block
                    let out_acc = get_acc_offset_shared(spm_row, spm_stride, spm_height_row, range_row,             i * bdim_y);
                    let arr_acc = get_acc_offset_shared(spm_col, spm_stride, spm_height_col, range_row, range_col + i * bdim_y);
                    body(tid_x, tid_y, out_acc, arr_acc, mask_col, is_row);
                }
                if tid_x < range_row @{
                    // left halo
                    let out_acc = get_acc_offset_shared(spm_row, spm_stride, spm_height_row, 0,             i * bdim_y);
                    let arr_acc = get_acc_offset_shared(spm_col, spm_stride, spm_height_col, 0, range_col + i * bdim_y);
                    body(tid_x, tid_y, out_acc, arr_acc, mask_col, is_row);
                }
                if tid_x >= bdim_x-range_row @{
                    // right halo
                    let out_acc = get_acc_offset_shared(spm_row, spm_stride, spm_height_row, range_row + range_row,             i * bdim_y);
                    let arr_acc = get_acc_offset_shared(spm_col, spm_stride, spm_height_col, range_row + range_row, range_col + i * bdim_y);
                    body(tid_x, tid_y, out_acc, arr_acc, mask_col, is_row);
                }
            }

            acc_barrier();

            for i in range(0, unroll) {
                // index space: block
                let is_row  = true;
                let out_acc =        get_acc_offset(out_gpu,                             acc_bdimx() * acc_bidx(), acc_bdimy() * acc_bidy() * unroll + i * bdim_y);
                let arr_acc = get_acc_offset_shared(spm_row, spm_stride, spm_height_row, range_row,                                                    i * bdim_y);
                body(tid_x, tid_y, out_acc, arr_acc, mask_row, is_row);
            }
        }
        acc_sync(acc_dev());
    }

    copy(out_gpu.buf, out.buf, out.width * out.height * sizeof[f32]());
    release(arr_gpu.buf);
    release(out_gpu.buf);
}
